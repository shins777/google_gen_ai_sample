{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ur8xi4C7S06n"},"outputs":[],"source":["# Copyright 2024 Google LLC\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"JAPoU8Sm5E6e"},"source":["# Intro to Context Caching with the Gemini API\n","\n","<table align=\"left\">\n","  <td style=\"text-align: center\">\n","    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/context-caching/intro_context_caching.ipynb\">\n","      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n","    </a>\n","  </td>\n","  <td style=\"text-align: center\">\n","    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fcontext-caching%2Fintro_context_caching.ipynb\">\n","      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n","    </a>\n","  </td>    \n","  <td style=\"text-align: center\">\n","    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/context-caching/intro_context_caching.ipynb\">\n","      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n","    </a>\n","  </td>\n","  <td style=\"text-align: center\">\n","    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/context-caching/intro_context_caching.ipynb\">\n","      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n","    </a>\n","  </td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"84f0f73a0f76"},"source":["| | |\n","|-|-|\n","|Author(s) | [Eric Dong](https://github.com/gericdong)|"]},{"cell_type":"markdown","metadata":{"id":"tvgnzT1CKxrO"},"source":["## Overview\n","\n","### Gemini\n","\n","Gemini is a family of generative AI models developed by Google DeepMind that is designed for multimodal use cases.\n","\n","### Context Caching\n","\n","The Gemini API provides the context caching feature for developers to store frequently used input tokens in a dedicated cache and reference them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model. This feature can help reduce the number of tokens sent to the model, thereby lowering the cost of requests that contain repeat content with high input token counts.\n","\n","### Objectives\n","\n","In this tutorial, you learn how to use the Gemini API context caching feature in Vertex AI.\n","\n","You will complete the following tasks:\n","- Create a context cache\n","- Retrieve and use a context cache\n","- Use context caching in Chat\n","- Update the expire time of a context cache\n","- Delete a context cache\n"]},{"cell_type":"markdown","metadata":{"id":"61RBz8LLbxCR"},"source":["## Get started"]},{"cell_type":"markdown","metadata":{"id":"No17Cw5hgx12"},"source":["### Install Vertex AI SDK and other required packages\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tFy3H3aPgx12","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722994503157,"user_tz":-540,"elapsed":9870,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"5331f2fc-9093-4d80-f4e0-be123927bffe"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/5.1 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/5.1 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/root/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["%pip install --upgrade --user --quiet google-cloud-aiplatform"]},{"cell_type":"markdown","metadata":{"id":"R5Xep4W9lq-Z"},"source":["### Restart runtime\n","\n","To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n","\n","The restart might take a minute or longer. After it's restarted, continue to the next step."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XRvKdaPDTznN"},"outputs":[],"source":["import IPython\n","\n","app = IPython.Application.instance()\n","app.kernel.do_shutdown(True)"]},{"cell_type":"markdown","metadata":{"id":"SbmM4z7FOBpM"},"source":["<div class=\"alert alert-block alert-warning\">\n","<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n","</div>\n"]},{"cell_type":"markdown","metadata":{"id":"dmWOrTJ3gx13"},"source":["### Authenticate your notebook environment (Colab only)\n","\n","If you're running this notebook on Google Colab, run the cell below to authenticate your environment."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"NyKGtVQjgx13","executionInfo":{"status":"ok","timestamp":1723002807149,"user_tz":-540,"elapsed":43242,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["import sys\n","\n","if \"google.colab\" in sys.modules:\n","    from google.colab import auth\n","\n","    auth.authenticate_user()"]},{"cell_type":"markdown","metadata":{"id":"DF4l8DTdWgPY"},"source":["### Set Google Cloud project information and initialize Vertex AI SDK\n","\n","To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n","\n","Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Nqwi-5ufWp_B","executionInfo":{"status":"ok","timestamp":1723002811746,"user_tz":-540,"elapsed":4602,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["PROJECT_ID = \"ai-hangsik\"  # @param {type:\"string\"}\n","LOCATION = \"asia-northeast3\"  # @param {type:\"string\"}\n","\n","import vertexai\n","\n","vertexai.init(project=PROJECT_ID, location=LOCATION)"]},{"cell_type":"markdown","metadata":{"id":"EdvJRUWRNGHE"},"source":["## Code Examples"]},{"cell_type":"markdown","metadata":{"id":"vHwJCyNF6u0O"},"source":["### Import libraries"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"mginH0QC6u0O","executionInfo":{"status":"ok","timestamp":1723002811747,"user_tz":-540,"elapsed":5,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["import datetime\n","\n","import vertexai\n","from vertexai.generative_models import Part\n","from vertexai.preview import caching\n","from vertexai.preview.generative_models import GenerativeModel"]},{"cell_type":"markdown","metadata":{"id":"PAGPrpYagu2z"},"source":["### Create a context cache\n","\n","**Note**: Context caching is only available for stable models with fixed versions (for example, `gemini-1.5-pro-001`). You must include the version postfix (for example, the `-001` in `gemini-1.5-pro-001`).\n","\n","For more information, see [Available Gemini stable model versions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versioning#stable-versions-available).\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"XdpfXqpJ-NNj","executionInfo":{"status":"ok","timestamp":1723002811747,"user_tz":-540,"elapsed":4,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["MODEL_ID = \"gemini-1.5-pro-001\"  # @param {type:\"string\"}"]},{"cell_type":"markdown","metadata":{"id":"appJ7LCc_YCW"},"source":["Context caching is particularly well suited to scenarios where a substantial initial context is referenced repeatedly by shorter requests.\n","\n","- Cached content can be any of the MIME types supported by Gemini multimodal models. For example, you can cache a large amount of text, audio, or video. **Note**: The minimum size of a context cache is 32,769 tokens.\n","- The default expiration time of a context cache is 60 minutes. You can specify a different expiration time using the `ttl` (time to live) or the `expire_time` property.\n","\n","This example shows how to create a context cache using two large research papers stored in a Cloud Storage bucket, and set the `ttl` to 60 minutes.\n","\n","- Paper 1: [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805)\n","- Paper 2: [Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://arxiv.org/abs/2403.05530)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"UmJA6AvVujyZ","executionInfo":{"status":"ok","timestamp":1723002855824,"user_tz":-540,"elapsed":44080,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["system_instruction = \"\"\"\n","You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n","You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n","Now look at the research paper below, and answer the following questions in 1-2 sentences.\n","\"\"\"\n","\n","contents = [\n","    Part.from_uri(\n","        \"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n","        mime_type=\"application/pdf\",\n","    ),\n","    Part.from_uri(\n","        \"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n","        mime_type=\"application/pdf\",\n","    ),\n","]\n","\n","cached_content = caching.CachedContent.create(\n","    model_name=MODEL_ID,\n","    system_instruction=system_instruction,\n","    contents=contents,\n","    ttl=datetime.timedelta(minutes=60),\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"7e1dKGSLDg2q"},"source":["You can access the properties of the cached content as example below. You can use its `name` or `resource_name` to reference the contents of the context cache.\n","\n","**Note**: The `name` of the context cache is also referred to as cache ID."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"uRJPRtkKDk2b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723002855825,"user_tz":-540,"elapsed":6,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"c0719d51-17a8-43fa-9ec7-a6bb748bc2ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["4020870042311720960\n","projects/721521243942/locations/asia-northeast3/cachedContents/4020870042311720960\n","projects/ai-hangsik/locations/asia-northeast3/publishers/google/models/gemini-1.5-pro-001\n","2024-08-07 03:53:32.988805+00:00\n","2024-08-07 04:53:32.915216+00:00\n"]}],"source":["print(cached_content.name)\n","print(cached_content.resource_name)\n","print(cached_content.model_name)\n","print(cached_content.create_time)\n","print(cached_content.expire_time)"]},{"cell_type":"markdown","metadata":{"id":"d-f5gTEaCPkN"},"source":["### Retrieve and use a context cache\n","\n","You can use the property `name` or `resource_name` to reference the contents of the context cache. For example:\n","```\n","new_cached_content = caching.CachedContent(cached_content_name=cached_content.name)\n","```"]},{"cell_type":"markdown","metadata":{"id":"RQ1zMmFQ1BNj"},"source":["To use the context cache, you construct a `GenerativeModel` with the context cache."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"EPVyJIW1BaVj","executionInfo":{"status":"ok","timestamp":1723002855825,"user_tz":-540,"elapsed":4,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["model = GenerativeModel.from_cached_content(cached_content=cached_content)"]},{"cell_type":"markdown","metadata":{"id":"1kgfyCoGH_w0"},"source":["Then you can query the model with a prompt, and the cached content will be used as a prefix to the prompt."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"5dSDogewDAHB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723002892699,"user_tz":-540,"elapsed":36877,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"a35253e9-37cc-44dd-b7c5-8bceed84fac3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Both research papers share the goal of building highly capable multimodal AI systems, focusing on expanding AI models' capabilities to understand and reason over long-form content across different modalities. The first paper introduces the Gemini family of models, while the second paper details improvements to the Gemini family, specifically the Gemini 1.5 Pro model. \n","\n"]}],"source":["response = model.generate_content(\n","    \"What is the research goal shared by these research papers?\"\n",")\n","\n","print(response.text)"]},{"cell_type":"markdown","metadata":{"id":"tX7vHiybEWeJ"},"source":["### Use context caching in Chat\n","\n","You can use the context cache in a multi-turn chat session.\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"FYNZS5o0FoGR","executionInfo":{"status":"ok","timestamp":1723002892699,"user_tz":-540,"elapsed":4,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["chat = model.start_chat()"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"5U-6wGSFFx51","executionInfo":{"status":"ok","timestamp":1723002933965,"user_tz":-540,"elapsed":41270,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"97ed5030-90c0-40ac-8fb2-d8fcb88c579a"},"outputs":[{"output_type":"stream","name":"stdout","text":["The main change in mitigation for Gemini 1.5 Pro compared to Gemini 1.0 is the incorporation of new image-to-text SFT data to address the challenge of harm-inducing queries containing text and images. Additionally, there is a greater focus on analyzing the potential negative downstream impacts of long-context understanding across modalities. \n","\n"]}],"source":["prompt = \"\"\"\n","How do the approaches to responsible AI development and mitigation strategies in Gemini 1.5 evolve from those in Gemini 1.0?\n","\"\"\"\n","\n","response = chat.send_message(prompt)\n","\n","print(response.text)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"FFO_JgKNeCpK","executionInfo":{"status":"ok","timestamp":1723002987978,"user_tz":-540,"elapsed":54015,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9eb4f7b6-5cab-46fa-a5ef-6bbfb8ed681b"},"outputs":[{"output_type":"stream","name":"stdout","text":["The research papers highlight the need to improve evaluations of long-context models by creating more challenging benchmarks that require complex reasoning over long inputs, especially those combining different modalities. Additionally, continued research is needed to tackle \"hallucinations\" in LLM outputs and improve their high-level reasoning capabilities, such as causal understanding, logical deduction, and counterfactual reasoning. \n","\n"]}],"source":["prompt = \"\"\"\n","Given the advancements presented in Gemini 1.5, what are the key future research directions identified in both papers\n","for further improving multimodal AI models?\n","\"\"\"\n","\n","response = chat.send_message(prompt)\n","\n","print(response.text)"]},{"cell_type":"markdown","metadata":{"id":"h2VhjUmojQjg"},"source":["You can use `print(chat.history)` to print out the chat session history."]},{"cell_type":"markdown","metadata":{"id":"ORsGDdXXLwHK"},"source":["### Update the expiration time of a context cache\n","\n","\n","The default expiration time of a context cache is 60 minutes. To update the expiration time, update one of the following properties:\n","\n","`ttl` - The number of seconds and nanoseconds that the cache lives after it's created or after the `ttl` is updated before it expires. When you set the `ttl`, the cache `expire_time` is updated.\n","\n","`expire_time` - A Timestamp that specifies the absolute date and time when the context cache expires."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"WyiZoZHKI2Jr","executionInfo":{"status":"ok","timestamp":1723002990878,"user_tz":-540,"elapsed":2903,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"05f8d5ef-cc1b-47c8-d597-333770d617de"},"outputs":[{"output_type":"stream","name":"stdout","text":["2024-08-07 04:56:28.913732+00:00\n"]}],"source":["cached_content.update(ttl=datetime.timedelta(hours=1))\n","\n","cached_content.refresh()\n","\n","print(cached_content.expire_time)"]},{"cell_type":"markdown","metadata":{"id":"chd6_8YRxdIu"},"source":["### Delete a context cache\n","\n","You can remove content from the cache using the delete operation."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"XGzgTk6YzgSt","executionInfo":{"status":"ok","timestamp":1723002991984,"user_tz":-540,"elapsed":1110,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1bd2e41d-533b-439c-9bfd-5501e860ff79"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.base:Deleting CachedContent : projects/721521243942/locations/asia-northeast3/cachedContents/4020870042311720960\n"]}],"source":["cached_content.delete()"]}],"metadata":{"colab":{"toc_visible":true,"provenance":[{"file_id":"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/context-caching/intro_context_caching.ipynb","timestamp":1722953478228}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}