{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1btZCldJxz-DFxshEjVFBksH3cFZ7ZWzY","timestamp":1717886695950}],"toc_visible":true,"authorship_tag":"ABX9TyMynACwNNxCHW6U9JIjOrLA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Disclaimer & Copyright\n","\n","Copyright 2024 Forusone : shins777@gmail.com\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n","\n","https://www.apache.org/licenses/LICENSE-2.0\n","\n","Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."],"metadata":{"id":"OQJknXG7C9Dq"}},{"cell_type":"markdown","source":["# Gemini - Youtube video analysis\n","* This notebook explains how to use Gemini to understand images in multimodality features of Gemini. This code shows how to use Gemini to analyze a Youtube videos with the feature.\n","* The youtube video that is used in this demo is \"https://www.youtube.com/watch?v=nXVvvRhiGjI\", the usage of the videos is just for the educational purpose.\n","* The video owner is KI Campus, please contact them to get permission if you want to use it.\n","* Don't use this Youtube video for the other purpose.\n","* Refer to the link for more information about the Gemini\n"," * ***https://cloud.google.com/vertex-ai/generative-ai/docs/learn/overview***"],"metadata":{"id":"zB93wBDYDCPI"}},{"cell_type":"markdown","source":["# Configuration\n","## Install python packages\n","* Vertex AI SDK for Python\n","  * https://cloud.google.com/python/docs/reference/aiplatform/latest\n","* Vertex AI initialization : aiplatform.init(..)\n","  * https://cloud.google.com/python/docs/reference/aiplatform/latest#initialization\n","* Install pytube to download the Youtube video\n","  * https://github.com/pytube/pytube\n","  * pip install pytube"],"metadata":{"id":"Vuxu3iRDDtcl"}},{"cell_type":"code","source":["%pip install --upgrade --quiet google-cloud-aiplatform"],"metadata":{"id":"wevZD-jnD-ft","executionInfo":{"status":"ok","timestamp":1718364947863,"user_tz":-540,"elapsed":13328,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["%pip install -q -U pytube"],"metadata":{"id":"YWrHreozkIxS","executionInfo":{"status":"ok","timestamp":1718364962329,"user_tz":-540,"elapsed":14468,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from IPython.display import display, Markdown"],"metadata":{"id":"-nhL4T2sKsdp","executionInfo":{"status":"ok","timestamp":1718364962329,"user_tz":-540,"elapsed":3,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Authentication to access to the GCP & Google drive\n","\n","* Use OAuth to access the GCP environment.\n"," * Refer to the authentication methods in GCP : https://cloud.google.com/docs/authentication?hl=ko"],"metadata":{"id":"tCVttGUsEzgj"}},{"cell_type":"code","source":["#  For only colab to authenticate to get an access to the GCP.\n","import sys\n","\n","if \"google.colab\" in sys.modules:\n","    from google.colab import auth\n","    auth.authenticate_user()"],"metadata":{"id":"AJuo1g4bE3-x","executionInfo":{"status":"ok","timestamp":1718364962886,"user_tz":-540,"elapsed":559,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["* Mount to the google drive to access the .ipynb files in the repository.\n","\n"],"metadata":{"id":"mQoisCsVE6LM"}},{"cell_type":"code","source":["# To access contents in Google drive\n","\n","if \"google.colab\" in sys.modules:\n","  from google.colab import drive\n","  drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZhluEayrE_Io","executionInfo":{"status":"ok","timestamp":1718364982819,"user_tz":-540,"elapsed":19935,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"0d3700d5-3a3a-4383-9314-73caf6d1fb82"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Download the Youtube video to anlyze.\n","* The youtube video that is used in this demo is \"https://www.youtube.com/watch?v=nXVvvRhiGjI\", the usage of the videos is just for the educational purpose.\n","* The video owner is Google, please contact them to get permission if you want to use it."],"metadata":{"id":"ktnOTtoJD2wx"}},{"cell_type":"code","source":["from pytube import YouTube\n","\n","# Download youtube video to your local drive in colab.\n","YouTube('https://www.youtube.com/watch?v=nXVvvRhiGjI').streams.first().download()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":39},"id":"SYt7U21d_zob","executionInfo":{"status":"ok","timestamp":1718364986745,"user_tz":-540,"elapsed":3928,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"8c9247bc-697a-49bd-af02-be761539d0c5"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/Project Astra Our vision for the future of AI assistants.mp4'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["if \"google.colab\" in sys.modules:\n","\n","  import moviepy.editor\n","  moviepy.editor.ipython_display(\"Project Astra Our vision for the future of AI assistants.mp4\", maxduration = 150)"],"metadata":{"id":"b-OBKUvnHIJ2","executionInfo":{"status":"ok","timestamp":1718364988657,"user_tz":-540,"elapsed":1914,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# Execute the example\n","## Set the environment on GCP Project\n","* Configure project information\n","  * Model name : LLM model name : https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models\n","  * Project Id : prodect id in GCP\n","  * Region : region name in GCP"],"metadata":{"id":"ey_Bv55hIblt"}},{"cell_type":"code","source":["MODEL_NAME=\"gemini-1.5-flash\"\n","PROJECT_ID=\"ai-hangsik\"\n","REGION=\"asia-northeast3\""],"metadata":{"id":"XJwhurOUHIG-","executionInfo":{"status":"ok","timestamp":1718364988657,"user_tz":-540,"elapsed":3,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### Vertex AI initialization\n","Configure Vertex AI and access to the foundation model."],"metadata":{"id":"Xk3nZQQhIiom"}},{"cell_type":"code","source":["import vertexai\n","from vertexai.preview.generative_models import GenerativeModel, Part\n","import vertexai.preview.generative_models as generative_models\n","\n","# Initalizate the current vertex AI execution environment.\n","vertexai.init(project=PROJECT_ID, location=REGION)\n","\n","# Access to the generative model.\n","model = GenerativeModel(MODEL_NAME)"],"metadata":{"id":"HZ6WeWz4HIEW","executionInfo":{"status":"ok","timestamp":1718364992459,"user_tz":-540,"elapsed":3804,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Encoding function for multimodality"],"metadata":{"id":"2QV1mbbBJRGf"}},{"cell_type":"code","source":["import base64\n","\n","def get_encoded_content(location_type, location, mime_type ):\n","  \"\"\"\n","  Get the encoded content object.\n","\n","  location_type :\n","    The type of the location. ( local or GCS )\n","  location :\n","    The file location of the content.\n","  mime_type :\n","    The mime type of the content.\n","\n","  Returns:\n","    The encoded content object.\n","\n","  \"\"\"\n","\n","  content_obj = None\n","\n","  if location_type == \"local\":\n","    with open(location, 'rb') as f:\n","      raw_obj = base64.b64encode(f.read()).decode('utf-8')\n","      content_obj = Part.from_data(data=base64.b64decode(raw_obj), mime_type=mime_type)\n","\n","  elif location_type == \"GCS\":\n","        content_obj = Part.from_uri(location, mime_type=mime_type)\n","  else:\n","    raise ValueError(\"Invalid location type.\")\n","\n","  return content_obj"],"metadata":{"id":"ySzNqqGmHIBm","executionInfo":{"status":"ok","timestamp":1718364992459,"user_tz":-540,"elapsed":3,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### Function to get the response"],"metadata":{"id":"r4NbkJ4HJYBB"}},{"cell_type":"code","source":["def generate(content_obj, query:str):\n","    \"\"\"\n","    Generate a response from the model.\n","\n","    content_obj :\n","      encoded object being analyzed in the process\n","    query :\n","      query to be sent to the model\n","\n","    Returns:\n","      The generated response.\n","\n","    \"\"\"\n","\n","    # Set model parameter : https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/send-multimodal-prompts#set_model_parameters\n","    generation_config = {\n","        \"max_output_tokens\": 8192,\n","        \"temperature\": 1,\n","        \"top_p\": 0.95,\n","    }\n","\n","    # Configure satey setting : https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes\n","    # Refer to the link to remove : https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes#how_to_remove_automated_response_blocking_for_select_safety_attributes\n","    safety_settings = {\n","        generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n","        generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n","        generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n","        generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n","    }\n","\n","    responses = model.generate_content(\n","        [content_obj, query],\n","        generation_config=generation_config,\n","        safety_settings=safety_settings,\n","        stream=False,\n","    )\n","\n","    return responses.text"],"metadata":{"id":"sMaCwPVGJXX6","executionInfo":{"status":"ok","timestamp":1718364992459,"user_tz":-540,"elapsed":3,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["## Run example"],"metadata":{"id":"bGdT-2WHJuXf"}},{"cell_type":"code","source":["\n","from time import perf_counter\n","\n","t1_start = perf_counter()\n","\n","# When using local storage for the file location.\n","location_type = \"local\"\n","mime_type = \"video/mp4\"\n","\n","repository_root = \".\"\n","file_path = \"/Project Astra Our vision for the future of AI assistants.mp4\"\n","location = repository_root + file_path\n","\n","content_obj = get_encoded_content(location_type, location, mime_type )\n","\n","prompt = \"\"\"\n","You are a helpful assistant that finds the factual information in the video.\n","Follow the instructions below to generate a response.\n","\n","1. Transcribe this video in English and summarize it in Korean\n","2. What was the object beside an glasses on the desk?\n","3. What was the colour of the speaker?\n","4. What kind of system architecture was drawn on the whiteboard\n","\n","\"\"\"\n","\n","outcome = generate(content_obj, prompt)\n","\n","t1_end  = perf_counter()\n","print(f\"Time : {t1_end - t1_start} seconds\\n\\n\")\n","\n","display(Markdown(outcome))\n","\n","\n","# When using Google Cloud Storage for the location type.\n","\n","# location_type = \"GCS\"\n","# mime_type = \"video/mp4\"\n","# repository_root = \"gs://bucket_name\"\n","# file_path = \"/Project Astra Our vision for the future of AI assistants.mp4\"\n","# location = repository_root + file_path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":412},"id":"-0Lpj5TaJXVQ","executionInfo":{"status":"ok","timestamp":1718365026634,"user_tz":-540,"elapsed":34177,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"23aab950-e4c3-4e46-acfb-e7ba10f1b02b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Time : 34.129156400000056 seconds\n","\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Here is the transcription of the video:\n\nThis is a two-part demo of Project Astra, Google DeepMind’s vision for the future of AI assistants. Each part was captured in a single take, in real time.\n\nOkay. Let’s do some tests. Tell me when you see something that makes sound. I see a speaker, which makes sound. What is that part of the speaker called? That is the tweeter. It produces high-frequency sounds. Give me a creative alliteration about these. Creative crayons color cheerfully. They certainly craft colorful creations.  What does that part of the code do? This code defines encryption and decryption functions. It seems to use AES-CBC encryption to encode and decode data based on a key and an initialization vector (IV). That’s right. What neighborhood do you think I’m in? This appears to be the King’s Cross area of London. It is known for its railway station and transportation connections. Do you remember where you saw my glasses? Yes, I do. Your glasses were on the desk near a red apple. \n\n**Korean summary**: 구글 딥마인드가 인공지능 비서의 미래에 대한 비전을 보여주는 ‘Project Astra’의 데모 영상입니다. 영상은 두 부분으로 구성되며, 각 부분은 실시간으로 한 번에 촬영되었습니다. 영상에서 AI 비서는 소리가 나는 물체를 찾고, 스피커의 부품 이름을 설명하고, 색연필을 보고 관련된 문구를 만들어 냅니다. 또한, 암호화 코드를 분석하여 기능을 설명하고, 현재 위치를 추측하고, 사용자가 놓고 간 안경의 위치를 기억하여 알려줍니다. 이러한 기능들을 통해 Project Astra는 인공지능 비서가 사용자의 환경을 이해하고 도움을 줄 수 있는 가능성을 보여줍니다.\n\n\n**Object beside the glasses on the desk**: A red apple. \n\n**Speaker's colour**: White. \n\n**System architecture on the whiteboard**:  The system architecture was a simple client-server system. The client (user) sends a request to the NLB (Network Load Balancer). The NLB then forwards the request to one of the servers. The server processes the request and sends a response to the database (DB). The DB then sends a response back to the server, which then sends a response back to the client.  The presenter suggested that adding a cache between the server and the database could improve the system's speed. \n\n\n"},"metadata":{}}]}]}